{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jaws Segmentation Task\n",
    "\n",
    "Hello, You're tasked with building a neural network using pytorch which segments upper (Maxilla) and lower(Mandible) jaws in a DICOM file\n",
    "\n",
    "The Dataset is provided as 2D slices from all 3 orthogonal points of view Axial, Coronal, and Sagittal, if you're not familiar with what a DICOM is, you're expected to read online about it\n",
    "\n",
    "Duration: 1 Week\n",
    "\n",
    "Deliverables: This notebook (which should contain your solution), and any other files (like saved weights) in a **zip** file, and you mail this **zip** file to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --user torch torchvision matplotlib numpy progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "import progressbar\n",
    "from math import ceil\n",
    "import torch\n",
    "import gzip\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_DATASET_PATH = 'dataset'\n",
    "BATCH_SIZE = 16\n",
    "AXIAL_TRAINING_DATASET = 'https://cvml-datasets.s3.eu-west-3.amazonaws.com/jaws-segmentation/v1/public/2d/axial/train.zip'\n",
    "AXIAL_TESTING_DATASET = 'https://cvml-datasets.s3.eu-west-3.amazonaws.com/jaws-segmentation/v1/public/2d/axial/test.zip'\n",
    "CORONAL_TRAINING_DATASET = 'https://cvml-datasets.s3.eu-west-3.amazonaws.com/jaws-segmentation/v1/public/2d/coronal/train.zip'\n",
    "CORONAL_TESTING_DATASET = 'https://cvml-datasets.s3.eu-west-3.amazonaws.com/jaws-segmentation/v1/public/2d/coronal/test.zip'\n",
    "SAGITTAL_TRAINING_DATASET = 'https://cvml-datasets.s3.eu-west-3.amazonaws.com/jaws-segmentation/v1/public/2d/sagittal/train.zip'\n",
    "SAGITTAL_TESTING_DATASET = 'https://cvml-datasets.s3.eu-west-3.amazonaws.com/jaws-segmentation/v1/public/2d/sagittal/test.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Dataset\n",
    "\n",
    "In this part we download the publicly available dataset, you can skip it if you already have it, it should be 5.6 Gb worth of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_progress_bar = None\n",
    "def show_progress(block_num, block_size, total_size):\n",
    "    global download_progress_bar\n",
    "    if download_progress_bar is None:\n",
    "        download_progress_bar = progressbar.ProgressBar(maxval=total_size)\n",
    "        download_progress_bar.start()\n",
    "\n",
    "    downloaded = block_num * block_size\n",
    "    if downloaded < total_size:\n",
    "        download_progress_bar.update(downloaded)\n",
    "    else:\n",
    "        download_progress_bar.finish()\n",
    "        download_progress_bar = None\n",
    "\n",
    "def download_file(url, disk_path):\n",
    "    print(f'downloading {url}')\n",
    "    filename, _ = urllib.request.urlretrieve(url, reporthook=show_progress)\n",
    "    os.makedirs(disk_path)\n",
    "    with zipfile.ZipFile(filename, 'r') as zip:\n",
    "        zip.extractall(disk_path)\n",
    "\n",
    "def download_data(to=LOCAL_DATASET_PATH):\n",
    "    download_file(AXIAL_TRAINING_DATASET, os.path.join(to, 'axial', 'train'))\n",
    "    download_file(AXIAL_TESTING_DATASET, os.path.join(to, 'axial', 'test'))\n",
    "    download_file(CORONAL_TRAINING_DATASET, os.path.join(to, 'coronal', 'train'))\n",
    "    download_file(CORONAL_TESTING_DATASET, os.path.join(to, 'coronal', 'test'))\n",
    "    download_file(SAGITTAL_TRAINING_DATASET, os.path.join(to, 'sagittal', 'train'))\n",
    "    download_file(SAGITTAL_TESTING_DATASET, os.path.join(to, 'sagittal', 'test'))\n",
    "\n",
    "download_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore The Dataset\n",
    "\n",
    "In this section you should explore/plot the dataset and get familiar with it, we are nice enough to write a dataset loader for you and we did some initial visualization for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JawsDataset(torch.utils.data.Dataset):\n",
    "\tdef __init__(self, dicom_file_list, transforms):\n",
    "\t\tself.dicom_file_list = dicom_file_list\n",
    "\t\tself.transforms = transforms\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.dicom_file_list)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tdicom_path = self.dicom_file_list[idx]\n",
    "\t\tlabel_path = dicom_path.replace('.dicom.npy.gz', '.label.npy.gz')\n",
    "\t\tdicom_file = gzip.GzipFile(dicom_path, 'rb')\n",
    "\t\tdicom = np.load(dicom_file)\n",
    "\t\tlabel_file = gzip.GzipFile(label_path, 'rb')\n",
    "\t\tlabel = np.load(label_file)\n",
    "\t\treturn self.transforms(dicom), self.transforms(label)\n",
    "\n",
    "def axial_dataset_train(transforms, validation_ratio = 0.1):\n",
    "\tfiles = glob.glob('dataset/axial/train/**/*.dicom.npy.gz')\n",
    "\tassert len(files) > 0\n",
    "\tvalidation_files_count = ceil(len(files) * validation_ratio)\n",
    "\n",
    "\treturn (JawsDataset(files[validation_files_count:], transforms),\n",
    "\t\t\tJawsDataset(files[:validation_files_count], transforms))\n",
    "\n",
    "def coronal_dataset_train(transforms, validation_ratio = 0.1):\n",
    "\tfiles = glob.glob('dataset/coronal/train/**/*.dicom.npy.gz')\n",
    "\tassert len(files) > 0\n",
    "\tvalidation_files_count = ceil(len(files) * validation_ratio)\n",
    "\n",
    "\treturn (JawsDataset(files[validation_files_count:], transforms),\n",
    "\t\t\tJawsDataset(files[:validation_files_count], transforms))\n",
    "\n",
    "def sagittal_dataset_train(transforms, validation_ratio = 0.1):\n",
    "\tfiles = glob.glob('dataset/sagittal/train/**/*.dicom.npy.gz')\n",
    "\tassert len(files) > 0\n",
    "\tassert len(files) > 0\n",
    "\tvalidation_files_count = ceil(len(files) * validation_ratio)\n",
    "\n",
    "\treturn (JawsDataset(files[validation_files_count:], transforms),\n",
    "\t\t\tJawsDataset(files[:validation_files_count], transforms))\n",
    "\n",
    "def axial_dataset_test(transforms):\n",
    "\tfiles = glob.glob('dataset/axial/test/**/*.dicom.npy.gz')\n",
    "\tassert len(files) > 0\n",
    "\treturn JawsDataset(files, transforms)\n",
    "\n",
    "def coronal_dataset_test(transforms):\n",
    "\tfiles = glob.glob('dataset/coronal/test/**/*.dicom.npy.gz')\n",
    "\tassert len(files) > 0\n",
    "\treturn JawsDataset(files, transforms)\n",
    "\n",
    "def sagittal_dataset_test(transforms):\n",
    "\tfiles = glob.glob('dataset/sagittal/test/**/*.dicom.npy.gz')\n",
    "\tassert len(files) > 0\n",
    "\treturn JawsDataset(files, transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_transforms = transforms.Compose([transforms.ToTensor(), transforms.Resize((128, 128))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axial_train_dataset, axial_validatiaon_dataset = axial_dataset_train(dataset_transforms)\n",
    "coronal_train_dataset, coronal_validation_dataset = coronal_dataset_train(dataset_transforms)\n",
    "sagittal_train_dataset, sagittal_validation_dataset = sagittal_dataset_train(dataset_transforms)\n",
    "print(f'axial training dataset: {len(axial_train_dataset)} slice')\n",
    "print(f'coronal training dataset: {len(coronal_train_dataset)} slice')\n",
    "print(f'sagittal training dataset: {len(sagittal_train_dataset)} slice')\n",
    "\n",
    "axial_train_loader = torch.utils.data.DataLoader(axial_train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "coronal_train_loader = torch.utils.data.DataLoader(coronal_train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "sagittal_train_loader = torch.utils.data.DataLoader(sagittal_train_dataset, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axial_data_iter = iter(axial_train_loader)\n",
    "images, labels = axial_data_iter.next()\n",
    "plt.figure(figsize=(16, 4))\n",
    "for index in range(0, min(16, len(images))):\n",
    "\tplt.subplot(2, 8, index + 1)\n",
    "\tplt.axis('off')\n",
    "\tplt.imshow(images[index].numpy().squeeze(), cmap='bone')\n",
    "\tplt.imshow(labels[index].numpy().squeeze(), alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coronal_data_iter = iter(coronal_train_loader)\n",
    "images, labels = coronal_data_iter.next()\n",
    "plt.figure(figsize=(16, 4))\n",
    "for index in range(0, min(16, len(images))):\n",
    "\tplt.subplot(2, 8, index + 1)\n",
    "\tplt.axis('off')\n",
    "\tplt.imshow(images[index].numpy().squeeze(), cmap='bone')\n",
    "\tplt.imshow(labels[index].numpy().squeeze(), alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagittal_data_iter = iter(sagittal_train_loader)\n",
    "images, labels = sagittal_data_iter.next()\n",
    "plt.figure(figsize=(16, 4))\n",
    "for index in range(0, min(16, len(images))):\n",
    "\tplt.subplot(2, 8, index + 1)\n",
    "\tplt.axis('off')\n",
    "\tplt.imshow(images[index].numpy().squeeze(), cmap='bone')\n",
    "\tplt.imshow(labels[index].numpy().squeeze(), alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now that you have downloaded the dataset and inspected it for a bit, you should train your model here, you can train a single model which works with all views (axial, sagittal, coronal), or you can train a model each, it's up to you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Now you should run your trained model on the test dataset available below and report your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axial_test_dataset = axial_dataset_test(dataset_transforms)\n",
    "coronal_test_dataset = coronal_dataset_test(dataset_transforms)\n",
    "sagittal_test_dataset = sagittal_dataset_test(dataset_transforms)\n",
    "print(f'axial testing dataset: {len(axial_test_dataset)} slice')\n",
    "print(f'coronal testing dataset: {len(coronal_test_dataset)} slice')\n",
    "print(f'sagittal testing dataset: {len(sagittal_test_dataset)} slice')\n",
    "\n",
    "axial_test_loader = torch.utils.data.DataLoader(axial_train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "coronal_test_loader = torch.utils.data.DataLoader(coronal_train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "sagittal_test_loader = torch.utils.data.DataLoader(sagittal_train_dataset, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "756e5570d4491bc9653f2b1746fdac04e7f8f2c613af5a9093a879035790827d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
